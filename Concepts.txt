A model in RL means: Transition function & Reward function = model = knowing how the world works.
MODEL-FREE RL : I DONâ€™T know how the world works. I only learn by trial & error. algorithms value based and policy based MODEL-BASED RL : I learn a model of how the world works, then I PLAN inside it.2. You LEARN the model from experience
Called World Models / Learned Dynamics.
Model-based RL planners (like MCTS, MPC, etc.) are slow but smart.
Model-free RL policies (like a neural net Ï€(a|s)) are fast but dumb at the beginning.
Distillation means:
Use a powerful but expensive planner to produce very good decisions,
and then train a neural network policy to imitate those decisions.
Over time, the neural network becomes almost as good as the planner,
but much faster.
AlphaZero uses Monte Carlo Tree Search (MCTS) as a planner.AlphaZero trains its neural network to imitate the planner:
âœ… What is Policy Optimization?
In Policy Optimization, you directly learn the policy:
This means the neural network outputs a probability distribution over actions (stochastic policy) or a specific action (deterministic policy).
Instead of learning Q-values and then deriving a policy,
you learn the policy itself.
The goal is to maximize expected return: Why is it almost always ON-POLICY? Because the gradient of the objective  only makes sense for the data generated by the current policy.
If the data was generated by an old policy, it gives the wrong gradient â†’ leads to divergence

p(sâ€²,râˆ£s,a) is world model 
âœ… 1. Pure Planning (MPC) : 
How does it work?
You observe the current state.
You use the model to simulate the future for the next H steps.
You find the best action sequence (by optimizing future reward).
You execute only the first action.
Throw away the plan.
Replan again next step.
This is model-predictive control (MPC).

Imagination-Augmented Agents
for Deep Reinforcement Learning most advanced technique

ğŸ“¦ Architecture Breakdown
1. World Model (Environment Model)
A neural network that predicts:
p(s_{t+1}, r_t | s_t, a_t)
Given:
current state s
action a
Predicts:
next state
reward
This model is trained supervised using real environment data.
2. Imagination Core
This module runs the world model forward for several possible actions:
simulate(s_t, a):
    s1, r1 = model(s_t, a)
    s2, r2 = model(s1, Ï€(s1))
    ...
It generates imagined rollouts for each action.
These imagined trajectories produce a summary embedding of the â€œfutureâ€.
3. Aggregator
A small network compresses the imaginary rollouts into a compact vector (like a mental plan):
imagination_embedding = f(rollout1, rollout2, rollout3, ...)
4. Policy Network
The agent gets two inputs:
(a) Real state features
â€“ from CNN / MLP
(b) Imagined future summary
â€“ output of the imagination core
Together, they produce:
Ï€(a|s) â†’ policy
V(s) â†’ value function

SketchRNN [36] is an example of a MDN-RNN used to predict the next pen strokes of a sketch drawing. We use a similar model to predict the next latent vector 
z
z.
This approach is known as a Mixture Density Network [37, 38] combined with a RNN (MDN-RNN) Great question.
This is the core reason why world models (and SketchRNN) use probabilistic (MDN) instead of deterministic predictions.

Let me explain it with perfect clarity.

âœ… 1. The world is not deterministic

In RL environments, many futures are possible, even from the same state and action.

Example: In a car racing game in World Models:

The car might slide left a little

Or slide right a little

Or go perfectly straight

Even if same state + same action, the next frame can vary because:

physics noise

small engine randomness

floating point chaos

unobserved variables (e.g., hidden obstacles)

A deterministic model has to choose only ONE future, but in reality multiple futures are possible.

âŒ Deterministic model fails

If you force an RNN to predict exactly one future:

Case 1 â€” multiple real futures exist

The RNN learns the average of all futures, which is not a real future.

Example:
Two possible next states: (left) or (right)

Deterministic model takes average â†’ predicts the agent stays in the center.

This destroys structure and makes the world model useless.

âš ï¸ Example (very famous outcome)

If you train a deterministic RNN to predict the next video frame:

if ball can go left or right

the average is: blur

This is exactly why pixel prediction becomes blurry in deterministic networks.

âœ”ï¸ Probabilistic MDN solves this

MDN outputs a mixture of possible futures:

possibility 1: ball goes left

possibility 2: ball goes right

possibility 3: ball stays still

Instead of averaging them, the network learns:

p(z_{t+1} | z_t, a_t) = mixture of Gaussians


So it can represent multiple modes.

This is called multi-modal prediction.
ğŸ§  Why latent-space prediction is probabilistic

Even after compressing frames into latents, the future is still uncertain.

Example:
Latent vector z summarizes a frame.
But z throws away many micro-details.

Prediction becomes stochastic, not deterministic.

Thus MDN-RNN is used to model the distribution:

z_{t+1} is not a single point  
it is a sample from a Gaussian mixture

ğŸŸ¦ Summary Table
Feature	Deterministic RNN	Probabilistic MDN-RNN
Predicts	1 future	many futures
Handles uncertainty	âŒ no	âœ”ï¸ yes
Multi-modal	âŒ impossible	âœ”ï¸ mixture of Gaussians
Training result	blurry averages	sharp stochastic samples
Works for world models	âŒ fails	âœ”ï¸ succeeds
ğŸ¯ Simple Intuition
Deterministic â†’ â€œTell me ONE future.â€
Probabilistic â†’ â€œTell me ALL the likely futures.â€

World Models need the second.

ğŸ”¥ Final Answer

We use probabilistic MDN-RNN instead of a deterministic RNN because:

âœ”ï¸ The world has multiple possible futures
âœ”ï¸ Deterministic models average them â†’ wrong and blurry
âœ”ï¸ MDN can represent different possible outcomes
âœ”ï¸ RL agents need uncertainty to explore correctly
âœ”ï¸ Stochastic prediction = better world model = better planning
ğŸ¯ 1. How many Gaussian distributions?

You choose a number K (hyperparameter).

Examples:

SketchRNN uses K = 20.

World Models paper used K = 5â€“10.

MDN-RNN can use K = 3, 5, 10, 20, etc.

So the model outputs K Gaussian distributions.
So total output parameters:

means: Î¼1, Î¼2, â€¦, Î¼K
stds:  Ïƒ1, Ïƒ2, â€¦, ÏƒK
mixing probabilities: Ï€1, Ï€2, â€¦, Ï€K   (sum to 1)

Each Gaussian represents one possible next state.
âœ… World Model = 3 Parts

A â€œWorld Modelâ€ (as used in the Ha & Schmidhuber 2018 â€“ World Models paper) has three components:

1ï¸âƒ£ V (Vision Model) â€” compress images â†’ latent vector z

A VAE (Variational Autoencoder)

Takes a raw image (84Ã—84 frame)

Compresses it into a low-dimensional latent vector 

This makes â€œthe worldâ€ much simpler and smaller

âœ” This is not MDN-RNN.
This is the encoder part.

2ï¸âƒ£ M (Memory Model) â€” RNN predicts next latent vector
This is where MDN-RNN comes in.

Takes current latent vector 
 and action
Predicts distribution over next latent state 
Uses mixture density (Gaussian Mixture â†’ MDN)

Uses RNN (LSTM) to model temporal dependencies

âœ” MDN-RNN = the world dynamics model
âœ” It predicts what will happen next probabilistically

3ï¸âƒ£ C (Controller) â€” chooses actions based on imagined future

Very small policy network (even linear)

Takes as input
 is RNN hidden state (memory)

Outputs action 
âœ” Controller is trained inside the dreamed world
âœ” Uses the MDN-RNN predictions for training
3ï¸âƒ£ C (Controller) â€” chooses actions based on imagined future

Very small policy network (even linear)â€‹

âœ” Controller is trained inside the dreamed world
âœ” Uses the MDN-RNN predictions for training
âœ… What is the Controller (C)?

It is the policy of the World Model system.

But unlike PPO, A2C, SAC, etcâ€¦
the Controller is intentionally made extremely tiny â€” just a single linear layer.

Why?
Because the complexity of intelligence should be inside the world model (V + M), not inside the policy.
ğŸ§  Inputs to the Controller

At each timestep 
ğ‘¡
t, the controller receives:
â€‹zt

This is the compressed latent state from the VAE (Vision model)

Represents what the agent "sees"
ht

This is the hidden state from the MDN-RNN (Memory model)

Represents what the agent "remembers" and its predicted future

So the policy has access to:

â€œWhat I see (z)â€ + â€œWhat I predict (h)â€
