The goal of reinforcement learning (Sutton and Barto, 1998)
is to learn good policies for sequential decision problems,
by optimizing a cumulative future reward signal. 

If the maximum reward you can ever get from now on is small,
but the Q-value becomes very large â†’ itâ€™s unrealistic.
Example:

Maximum reward per step = 1
Episode length = 100
So the biggest possible return is 100.

If Q-learning suddenly gives you:
Q(s, a) = 500
That is impossible â†’ overestimated.

QÏ€(s,a)=E[R1â€‹+Î³R2â€‹+Î³2R3â€‹+â€¦âˆ£s0â€‹=s,a0â€‹=a,then follow Ï€]
QÏ€(s,a) is the average total reward you will get if you start with action a and then follow Ï€
Qâˆ—(s,a)=Ï€maxâ€‹QÏ€(s,a)
ğŸ‘‰ Q*(s,a) is the best possible value of action a
An optimal policy is easily derived from the optimal values by selecting the highest-valued action in each state.
Temporal Difference = learn from the difference between what you predicted and what actually happened next.
Estimates for the optimal action values can be learned using Q-learning, a form of temporal difference learning.
Q(s,a)â†Q(s,a)+Î±[r+Î³aâ€²maxâ€‹Q(sâ€²,aâ€²)âˆ’Q(s,a)]
New Q = Old Q + learning_rate Ã— (Better Guess âˆ’ Old Q)

Parametrized Q learning:
Q(s,a;Î¸) : The predicted value of taking action a in state s, computed by a neural network with parameters Î¸. rather than storing all values we store a function parameter values that generate those values
Î¸t+1â€‹=Î¸tâ€‹+Î±(YtQâ€‹âˆ’Q(Stâ€‹,Atâ€‹;Î¸tâ€‹))âˆ‡Î¸tâ€‹â€‹Q(Stâ€‹,Atâ€‹;Î¸tâ€‹)  YtQâ€‹âˆ’Q = error of prediction 
L=(YtQâ€‹âˆ’Q(Stâ€‹,Atâ€‹;Î¸))2
Î¸â†Î¸âˆ’Î±âˆ‡Î¸â€‹L
gradient: âˆ‡Î¸â€‹L=âˆ’2(YtQâ€‹âˆ’Q)âˆ‡Î¸â€‹Q
Standard Q-learning with neural networks is unstable and often diverges (blows up).
DQN adds two stabilizing mechanisms:
1ï¸âƒ£ Target network
2ï¸âƒ£ Experience replay
Target network : 
You keep TWO networks: (a) Online network: parameters: Î¸ used for learning and choosing actions , (b) Target network: parameters: Î¸â» used to compute targets Y
Î¸â» is updated only once every Ï„ steps . Î¸tâˆ’â€‹=Î¸tâ€‹every Ï„ steps
Because the target becomes stable for many steps.
Instead of chasing a moving target, the online network learns according to a fixed target network.
This prevents the "moving target problem."
YtDQNâ€‹â‰¡Rt+1â€‹+Î³amaxâ€‹Q(St+1â€‹,a;Î¸tâˆ’â€‹) instead of YtQâ€‹=Rt+1â€‹+Î³amaxâ€‹Q(St+1â€‹,a;Î¸tâ€‹)
Experience replay : Instead of learning from sequential data ,DQN stores all experience in a buffer: D={(Stâ€‹,Atâ€‹,Rt+1â€‹,St+1â€‹)} And each update uses a random mini-batch from this buffer.

ğŸŸ¦ Original Double Q-learning Algorithm
It maintains two value functions:
Î¸ (first Q-network)
Î¸â€² (second Q-network)
At each update:
You randomly choose one network to update
The other is used for the evaluation step
This creates independence.
YtDoubleQâ€‹=Rt+1â€‹+Î³Q(St+1â€‹,selection using Î¸ argamaxâ€‹Q(St+1â€‹,a;Î¸)â€‹â€‹,Î¸â€²)
Thrun & Schwartz (1993) result : 
If each Q-value has random uniform error in [âˆ’Îµ,Îµ] and there are m actions then Q-learning overestimates by approximately: Î³Ïµm+1/mâˆ’1â€‹ More actions â†’ more overestimation
What is the expectation of the maximum of m independent Uniform[-Îµ, Îµ] random variables? This is a known mathematical result: E[max(X1â€‹,...,Xmâ€‹)] = Ïµ*m+1/mâˆ’1
They create a simple toy environment with:
A continuous state space (real numbers).
10 discrete actions per state.
The true Q* values for all actions in that state are the same.
Q\*(s,1)=Q\*(s,2)....
Thus the optimal value function is just a function of the state, not of the action. Two different shapes are considered:
Case 1: Sinusoidal Q\*(s,a)=sin(s) Case 2: Gaussian bump Q\*(s,a)=2eâˆ’s2 We can see overestimation here as many approximations will be there and we use max
X-axis Training steps (in millions) , Y-axis Value estimates Orange line = DQN estimated value Blue line = Double DQN estimated value 
Straight horizontal orange line = true value (after training) for DQN
Straight horizontal blue line = true value for Double DQN
These plots show:
DQNâ€™s Q-values increase too much (overestimation).
Double DQN stays stable.
âœ… Step 1 â€” What is â€œnormalized performanceâ€?
For each Atari game:
They take the agentâ€™s score and convert it to a normalized scale:
Normalized Score = Agent score - Random score / Human score - Random score Ã— 100%
The table aggregates the 49 normalized scores In the median game:
DQN performs at 93.5% of human performance
Double DQN performs at 114.7% of human performance
Why Pong works even with overestimation? 
Because Pong is simple:
few states
few actions
Q-value noise doesnâ€™t change best action
But in many Atari games:
many actions
many states
small Q errors â†’ wrong action chosen
Thatâ€™s why DQN collapses in hard games.
Why they introduce a second evaluation: â€œHuman Startsâ€ ?
Problem with normal evaluation:
In Atari games with fixed start state:
agent may memorize action sequence
no real generalization
not robust
To fix this, they use:
Human Starts Evaluation
Take 100 states from a human expertâ€™s gameplay trajectory
Start the agent from each of those states
Only count rewards after that point
This tests generalization, not memorization.
This is a much harder test.
